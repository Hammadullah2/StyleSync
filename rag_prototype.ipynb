{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Multi-Modal RAG Prototype\n",
                "\n",
                "This notebook demonstrates a Multi-Modal Retrieval-Augmented Generation (RAG) system for a clothing website. It uses CLIP embeddings to search for images based on text queries."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies if not already installed\n",
                "!pip install boto3 pandas pillow open_clip_torch chromadb python-dotenv matplotlib langsmith langchain-google-genai"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import boto3\n",
                "import pandas as pd\n",
                "import os\n",
                "from PIL import Image\n",
                "import io\n",
                "import open_clip\n",
                "import torch\n",
                "import numpy as np\n",
                "import chromadb\n",
                "import matplotlib.pyplot as plt\n",
                "from langsmith import traceable\n",
                "\n",
                "# Configuration (Hardcoded for Colab)\n",
                "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"YOUR_AWS_ACCESS_KEY_ID\"\n",
                "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"YOUR_AWS_SECRET_ACCESS_KEY\"\n",
                "os.environ[\"S3_BUCKET_NAME\"] = \"stylesync-mlops-data\"\n",
                "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
                "os.environ[\"LANGCHAIN_PROJECT\"] = \"StyleSync\"\n",
                "os.environ[\"LANGCHAIN_API_KEY\"] = \"YOUR_LANGCHAIN_API_KEY\"\n",
                "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
                "os.environ[\"HF_TOKEN\"] = \"YOUR_HF_TOKEN\"\n",
                "os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_GOOGLE_API_KEY\"\n",
                "\n",
                "bucket_name = os.environ[\"S3_BUCKET_NAME\"]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Loading from S3"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "s3 = boto3.client('s3')\n",
                "\n",
                "def get_image_from_s3(filename):\n",
                "    try:\n",
                "        key = f\"style-sync/raw/fashion/images/{filename}\"\n",
                "        obj = s3.get_object(Bucket=bucket_name, Key=key)\n",
                "        img_data = obj['Body'].read()\n",
                "        return Image.open(io.BytesIO(img_data))\n",
                "    except Exception as e:\n",
                "        # print(f\"Error loading {filename}: {e}\")\n",
                "        return None\n",
                "\n",
                "print(f\"Connected to bucket: {bucket_name}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Styles CSV\n",
                "print(\"Loading styles.csv...\")\n",
                "obj = s3.get_object(Bucket=bucket_name, Key=\"style-sync/raw/fashion/styles.csv\")\n",
                "df = pd.read_csv(obj['Body'], on_bad_lines='skip')\n",
                "print(f\"Total items: {len(df)}\")\n",
                "\n",
                "# Sample a subset for the prototype\n",
                "SAMPLE_SIZE = 100\n",
                "df_sample = df.head(SAMPLE_SIZE).copy()\n",
                "print(f\"Using sample of {len(df_sample)} items\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Embedding Generation (CLIP)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Loading CLIP model...\")\n",
                "model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')\n",
                "tokenizer = open_clip.get_tokenizer('ViT-B-32')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_image_embedding(image):\n",
                "    image_tensor = preprocess(image).unsqueeze(0)\n",
                "    with torch.no_grad():\n",
                "        image_features = model.encode_image(image_tensor)\n",
                "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
                "    return image_features.cpu().numpy().flatten()\n",
                "\n",
                "def generate_text_embedding(text):\n",
                "    text_tokens = tokenizer([text])\n",
                "    with torch.no_grad():\n",
                "        text_features = model.encode_text(text_tokens)\n",
                "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
                "    return text_features.cpu().numpy().flatten()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Vector Database (ChromaDB)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "client = chromadb.Client()\n",
                "collection_name = \"fashion_items\"\n",
                "\n",
                "try:\n",
                "    client.delete_collection(name=collection_name)\n",
                "except:\n",
                "    pass\n",
                "\n",
                "collection = client.create_collection(name=collection_name)\n",
                "print(\"Created ChromaDB collection\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Indexing images...\")\n",
                "count = 0\n",
                "for idx, row in df_sample.iterrows():\n",
                "    img_filename = f\"{row['id']}.jpg\"\n",
                "    img = get_image_from_s3(img_filename)\n",
                "    \n",
                "    if img:\n",
                "        embedding = generate_image_embedding(img)\n",
                "        \n",
                "        # Metadata\n",
                "        metadata = {\n",
                "            \"id\": str(row['id']),\n",
                "            \"productDisplayName\": str(row['productDisplayName']),\n",
                "            \"articleType\": str(row['articleType']),\n",
                "            \"baseColour\": str(row['baseColour'])\n",
                "        }\n",
                "        \n",
                "        collection.add(\n",
                "            embeddings=[embedding.tolist()],\n",
                "            documents=[row['productDisplayName']],\n",
                "            metadatas=[metadata],\n",
                "            ids=[str(row['id'])]\n",
                "        )\n",
                "        count += 1\n",
                "        if count % 10 == 0:\n",
                "            print(f\"Indexed {count} items...\")\n",
                "\n",
                "print(f\"Finished indexing {count} items.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Search & Retrieval"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@traceable(run_type=\"retriever\")\n",
                "def search_products(query_text, n_results=3):\n",
                "    query_embedding = generate_text_embedding(query_text)\n",
                "    \n",
                "    results = collection.query(\n",
                "        query_embeddings=[query_embedding.tolist()],\n",
                "        n_results=n_results\n",
                "    )\n",
                "    \n",
                "    return results\n",
                "\n",
                "def display_results(results):\n",
                "    ids = results['ids'][0]\n",
                "    metadatas = results['metadatas'][0]\n",
                "    distances = results['distances'][0]\n",
                "    \n",
                "    for i, (id, meta, dist) in enumerate(zip(ids, metadatas, distances)):\n",
                "        print(f\"Rank {i+1}: {meta['productDisplayName']} (Distance: {dist:.4f})\")\n",
                "        img = get_image_from_s3(f\"{id}.jpg\")\n",
                "        if img:\n",
                "            plt.figure(figsize=(3,3))\n",
                "            plt.imshow(img)\n",
                "            plt.axis('off')\n",
                "            plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test Query\n",
                "query = \"red shoes\"\n",
                "print(f\"Searching for: {query}\")\n",
                "results = search_products(query)\n",
                "display_results(results)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Generation (Google Gemini)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_google_genai import ChatGoogleGenerativeAI\n",
                "from langchain_core.prompts import ChatPromptTemplate\n",
                "from langchain_core.output_parsers import StrOutputParser\n",
                "\n",
                "# Setup Gemini\n",
                "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", google_api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
                "\n",
                "def generate_response(query, results):\n",
                "    # Format context from results\n",
                "    context_items = []\n",
                "    ids = results['ids'][0]\n",
                "    metadatas = results['metadatas'][0]\n",
                "    \n",
                "    for i, (id, meta) in enumerate(zip(ids, metadatas)):\n",
                "        item_desc = f\"Item {i+1}: {meta['productDisplayName']} (Color: {meta['baseColour']}, Type: {meta['articleType']})\"\n",
                "        context_items.append(item_desc)\n",
                "    \n",
                "    context = \"\\n\".join(context_items)\n",
                "    \n",
                "    # Create Prompt\n",
                "    template = \"\"\"\n",
                "    You are a helpful fashion assistant for a clothing website.\n",
                "    Based on the user's query and the retrieved products, recommend the items and explain why they match.\n",
                "    \n",
                "    User Query: {query}\n",
                "    \n",
                "    Retrieved Products:\n",
                "    {context}\n",
                "    \n",
                "    Response:\n",
                "    \"\"\"\n",
                "    \n",
                "    prompt = ChatPromptTemplate.from_template(template)\n",
                "    chain = prompt | llm | StrOutputParser()\n",
                "    \n",
                "    return chain.invoke({\"query\": query, \"context\": context})\n",
                "\n",
                "    # Test Generation\n",
                "print(\"Generating response...\")\n",
                "response = generate_response(query, results)\n",
                "print(\"\\nAI Response:\")\n",
                "print(response)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}