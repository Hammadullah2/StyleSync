{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# StyleSync Ingestion Pipeline (Colab)\n",
                "\n",
                "This notebook performs the heavy lifting of ingesting 44k images, generating embeddings using CLIP, and storing them in a ChromaDB vector store. The result is zipped and uploaded to S3."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install boto3 pandas pillow open_clip_torch langchain-chroma tqdm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import boto3\n",
                "import pandas as pd\n",
                "import io\n",
                "from PIL import Image\n",
                "import torch\n",
                "import open_clip\n",
                "from langchain_chroma import Chroma\n",
                "from tqdm.notebook import tqdm\n",
                "import concurrent.futures\n",
                "\n",
                "# Configuration (Hardcoded for Colab)\n",
                "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"YOUR_AWS_ACCESS_KEY_ID\"\n",
                "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"YOUR_AWS_SECRET_ACCESS_KEY\"\n",
                "os.environ[\"S3_BUCKET_NAME\"] = \"stylesync-mlops-data\"\n",
                "os.environ[\"HF_TOKEN\"] = \"YOUR_HF_TOKEN\"\n",
                "\n",
                "S3_BUCKET = os.environ[\"S3_BUCKET_NAME\"]\n",
                "CHROMA_DB_DIR = \"./chroma_db\"\n",
                "MODEL_NAME = \"ViT-B-32\"\n",
                "CHECKPOINT = \"laion2b_s34b_b79k\"\n",
                "BATCH_SIZE = 32\n",
                "MAX_WORKERS = 8"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "s3 = boto3.client('s3')\n",
                "\n",
                "def load_styles_csv():\n",
                "    print(\"Loading styles.csv from S3...\")\n",
                "    obj = s3.get_object(Bucket=S3_BUCKET, Key=\"style-sync/raw/fashion/styles.csv\")\n",
                "    df = pd.read_csv(obj['Body'], on_bad_lines='skip')\n",
                "    print(f\"Loaded {len(df)} rows.\")\n",
                "    return df\n",
                "\n",
                "def process_metadata(df):\n",
                "    print(\"Processing metadata...\")\n",
                "    df['rich_caption'] = df.apply(\n",
                "        lambda x: f\"{x['productDisplayName']} {x['usage']} {x['season']} {x['baseColour']}\", \n",
                "        axis=1\n",
                "    )\n",
                "    return df\n",
                "\n",
                "df = load_styles_csv()\n",
                "df = process_metadata(df)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"Loading OpenCLIP model: {MODEL_NAME}...\")\n",
                "model, _, preprocess = open_clip.create_model_and_transforms(MODEL_NAME, pretrained=CHECKPOINT)\n",
                "model.eval()\n",
                "tokenizer = open_clip.get_tokenizer(MODEL_NAME)\n",
                "\n",
                "if torch.cuda.is_available():\n",
                "    model = model.cuda()\n",
                "    print(\"Using GPU\")\n",
                "else:\n",
                "    print(\"Using CPU\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class OpenCLIPEmbedder:\n",
                "    def __init__(self, model, tokenizer):\n",
                "        self.model = model\n",
                "        self.tokenizer = tokenizer\n",
                "        self.device = next(model.parameters()).device\n",
                "        \n",
                "    def embed_documents(self, texts):\n",
                "        with torch.no_grad():\n",
                "            text = self.tokenizer(texts).to(self.device)\n",
                "            text_features = self.model.encode_text(text)\n",
                "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
                "            return text_features.cpu().tolist()\n",
                "    \n",
                "    def embed_query(self, text):\n",
                "        return self.embed_documents([text])[0]\n",
                "\n",
                "embedding_function = OpenCLIPEmbedder(model, tokenizer)\n",
                "\n",
                "vectorstore = Chroma(\n",
                "    collection_name=\"style_sync\",\n",
                "    embedding_function=embedding_function,\n",
                "    persist_directory=CHROMA_DB_DIR\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def process_image_s3(row):\n",
                "    image_id = row['id']\n",
                "    s3_key = f\"style-sync/raw/fashion/images/{image_id}.jpg\"\n",
                "    try:\n",
                "        response = s3.get_object(Bucket=S3_BUCKET, Key=s3_key)\n",
                "        image_bytes = response['Body'].read()\n",
                "        image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
                "        return {\"id\": image_id, \"image\": image, \"row\": row, \"status\": \"success\"}\n",
                "    except Exception as e:\n",
                "        return {\"id\": image_id, \"status\": \"failed\", \"error\": str(e)}\n",
                "\n",
                "print(\"Starting ingestion...\")\n",
                "device = next(model.parameters()).device\n",
                "\n",
                "# Process in batches\n",
                "for i in tqdm(range(0, len(df), BATCH_SIZE), desc=\"Ingesting Batches\"):\n",
                "    batch_df = df.iloc[i:i+BATCH_SIZE]\n",
                "    \n",
                "    results = []\n",
                "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
                "        futures = [executor.submit(process_image_s3, row) for _, row in batch_df.iterrows()]\n",
                "        for future in concurrent.futures.as_completed(futures):\n",
                "            results.append(future.result())\n",
                "    \n",
                "    ids = []\n",
                "    metadatas = []\n",
                "    embeddings = []\n",
                "    texts = []\n",
                "    \n",
                "    for res in results:\n",
                "        if res['status'] == 'success':\n",
                "            try:\n",
                "                image = res['image']\n",
                "                image_input = preprocess(image).unsqueeze(0).to(device)\n",
                "                \n",
                "                with torch.no_grad():\n",
                "                    image_features = model.encode_image(image_input)\n",
                "                    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
                "                \n",
                "                ids.append(str(res['id']))\n",
                "                metadatas.append(res['row'].to_dict())\n",
                "                texts.append(res['row']['rich_caption'])\n",
                "                embeddings.append(image_features.cpu().squeeze().tolist())\n",
                "                \n",
                "            except Exception as e:\n",
                "                print(f\"Error embedding {res['id']}: {e}\")\n",
                "    \n",
                "    if embeddings:\n",
                "        vectorstore.add_texts(\n",
                "            texts=texts,\n",
                "            embeddings=embeddings,\n",
                "            metadatas=metadatas,\n",
                "            ids=ids\n",
                "        )\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Ingestion complete. Zipping database...\")\n",
                "!zip -r chroma_db.zip chroma_db"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Uploading to S3...\")\n",
                "s3.upload_file(\"chroma_db.zip\", S3_BUCKET, \"style-sync/artifacts/chroma_db.zip\")\n",
                "print(\"Upload complete! You can now run download_db.py locally.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}