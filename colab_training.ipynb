{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# StyleSync: Multi-Label Fashion Classification (Streaming from S3)\n",
                "\n",
                "This notebook trains a ResNet-50 model to classify fashion attributes using the Fashion Product Images Dataset.\n",
                "It uses `s3torchconnector` to stream images directly from S3, avoiding local disk storage constraints."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 1: Install dependencies\n",
                "# IMPORTANT: After running this cell, you MUST restart the runtime (Runtime -> Restart Runtime)\n",
                "# for the installation to take effect.\n",
                "!pip install s3torchconnector \"smart_open[s3]\" pandas matplotlib tqdm boto3"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "eb1b65f9",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 2: Imports & Auth\n",
                "import os\n",
                "import io\n",
                "import torch\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import boto3\n",
                "from PIL import Image\n",
                "from tqdm.auto import tqdm\n",
                "from google.colab import userdata\n",
                "from sklearn.preprocessing import MultiLabelBinarizer\n",
                "\n",
                "# AWS S3 Connector\n",
                "from s3torchconnector import S3MapDataset\n",
                "\n",
                "# Torchvision\n",
                "from torchvision import transforms, models\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "\n",
                "# Smart Open for S3 file handling\n",
                "from smart_open import open as s_open\n",
                "\n",
                "# Setup Credentials\n",
                "# .strip() removes accidental whitespace from copy-pasting\n",
                "os.environ['AWS_ACCESS_KEY_ID'] = userdata.get('AWS_ACCESS_KEY_ID').strip()\n",
                "os.environ['AWS_SECRET_ACCESS_KEY'] = userdata.get('AWS_SECRET_ACCESS_KEY').strip()\n",
                "os.environ['AWS_REGION'] = 'us-east-1'\n",
                "\n",
                "BUCKET_NAME = \"stylesync-mlops-data\"\n",
                "PREFIX = \"style-sync/raw/fashion\"\n",
                "STYLES_CSV_KEY = f\"s3://{BUCKET_NAME}/{PREFIX}/styles.csv\"\n",
                "IMAGES_PREFIX = f\"s3://{BUCKET_NAME}/{PREFIX}/images/\"\n",
                "\n",
                "print(\"AWS Credentials loaded and environment configured.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4dfd7258",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 3: Metadata Processing\n",
                "\n",
                "def load_metadata(csv_s3_uri):\n",
                "    print(f\"Downloading metadata from {csv_s3_uri}...\")\n",
                "    \n",
                "    # Create a boto3 session to explicitly pass credentials to smart_open\n",
                "    session = boto3.Session(\n",
                "        aws_access_key_id=os.environ['AWS_ACCESS_KEY_ID'],\n",
                "        aws_secret_access_key=os.environ['AWS_SECRET_ACCESS_KEY'],\n",
                "        region_name=os.environ['AWS_REGION']\n",
                "    )\n",
                "    \n",
                "    # Stream csv directly into pandas using smart_open with the boto3 client\n",
                "    # This ensures the credentials are used correctly\n",
                "    with s_open(csv_s3_uri, 'rb', transport_params={'client': session.client('s3')}) as f:\n",
                "        df = pd.read_csv(f, on_bad_lines='skip')\n",
                "    \n",
                "    # Clean dataset: Ensure we have an ID and some attributes\n",
                "    df = df.dropna(subset=['id', 'articleType', 'baseColour', 'season', 'usage'])\n",
                "    df['id'] = df['id'].astype(str)\n",
                "    \n",
                "    # Create a combined 'tags' column for multi-label classification\n",
                "    # We will predict: Article Type, Color, Season, Usage\n",
                "    df['tags'] = df.apply(lambda x: [\n",
                "        x['articleType'], \n",
                "        x['baseColour'], \n",
                "        x['season'], \n",
                "        x['usage']\n",
                "    ], axis=1)\n",
                "    \n",
                "    return df\n",
                "\n",
                "# Load Data\n",
                "df = load_metadata(STYLES_CSV_KEY)\n",
                "\n",
                "# Initialize MultiLabelBinarizer\n",
                "mlb = MultiLabelBinarizer()\n",
                "df['labels'] = list(mlb.fit_transform(df['tags']))\n",
                "\n",
                "CLASSES = mlb.classes_\n",
                "NUM_CLASSES = len(CLASSES)\n",
                "\n",
                "print(f\"Total Images: {len(df)}\")\n",
                "print(f\"Total Classes: {NUM_CLASSES}\")\n",
                "print(f\"Sample Classes: {CLASSES[:10]}\")\n",
                "print(f\"Sample Row:\\n{df.iloc[0][['id', 'tags']]}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a2dc1c28",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 4: Custom S3 Streaming Dataset\n",
                "\n",
                "class S3FashionDataset(Dataset):\n",
                "    def __init__(self, dataframe, bucket_prefix, transform=None):\n",
                "        \"\"\"\n",
                "        Args:\n",
                "            dataframe (pd.DataFrame): DataFrame containing 'id' and 'labels'.\n",
                "            bucket_prefix (str): S3 prefix for images (e.g., s3://bucket/path/images/).\n",
                "            transform (callable, optional): Transform to be applied on a sample.\n",
                "        \"\"\"\n",
                "        self.df = dataframe\n",
                "        self.bucket_prefix = bucket_prefix\n",
                "        self.transform = transform\n",
                "        \n",
                "        # Create a mapping of index -> S3 Key\n",
                "        # The S3MapDataset expects an iterable of S3 URIs\n",
                "        self.image_uris = [f\"{bucket_prefix}{row_id}.jpg\" for row_id in self.df['id']]\n",
                "        \n",
                "        # Initialize the S3 Connector\n",
                "        # We use S3MapDataset to handle the fetching logic efficiently\n",
                "        self.s3_dataset = S3MapDataset.from_objects(self.image_uris)\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.df)\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        # 1. Fetch Image Bytes from S3 (Streaming)\n",
                "        try:\n",
                "            s3_object = self.s3_dataset[idx]\n",
                "            image_bytes = s3_object.content\n",
                "            \n",
                "            # 2. Decode Image\n",
                "            image = Image.open(io.BytesIO(image_bytes)).convert('RGB')\n",
                "            \n",
                "            # 3. Apply Transforms\n",
                "            if self.transform:\n",
                "                image = self.transform(image)\n",
                "                \n",
                "            # 4. Get Labels\n",
                "            # Labels are pre-computed in the dataframe as numpy arrays\n",
                "            labels = torch.tensor(self.df.iloc[idx]['labels'], dtype=torch.float32)\n",
                "            \n",
                "            return image, labels\n",
                "            \n",
                "        except Exception as e:\n",
                "            print(f\"Error loading index {idx}: {e}\")\n",
                "            # Return a dummy tensor or handle error appropriately\n",
                "            # For simplicity, we'll return zeros (not recommended for prod, but prevents crash)\n",
                "            return torch.zeros((3, 224, 224)), torch.zeros(NUM_CLASSES)\n",
                "\n",
                "# Define Transforms\n",
                "train_transform = transforms.Compose([\n",
                "    transforms.Resize((224, 224)),\n",
                "    transforms.RandomHorizontalFlip(),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
                "])\n",
                "\n",
                "# Instantiate Dataset\n",
                "dataset = S3FashionDataset(df, IMAGES_PREFIX, transform=train_transform)\n",
                "\n",
                "# Quick Test\n",
                "print(f\"Dataset Size: {len(dataset)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3613e2d5",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 5: Model Definition\n",
                "\n",
                "class MultiLabelResNet(nn.Module):\n",
                "    def __init__(self, num_classes):\n",
                "        super(MultiLabelResNet, self).__init__()\n",
                "        # Load Pretrained ResNet50\n",
                "        self.resnet = models.resnet50(pretrained=True)\n",
                "        \n",
                "        # Replace the final Fully Connected Layer\n",
                "        # ResNet50's fc layer has 2048 input features\n",
                "        in_features = self.resnet.fc.in_features\n",
                "        self.resnet.fc = nn.Linear(in_features, num_classes)\n",
                "        \n",
                "    def forward(self, x):\n",
                "        return self.resnet(x)\n",
                "\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "model = MultiLabelResNet(num_classes=NUM_CLASSES)\n",
                "model = model.to(device)\n",
                "\n",
                "print(f\"Model initialized on {device}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3e8772dd",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 6: Training Loop & Checkpointing\n",
                "\n",
                "def save_checkpoint(state, filename=\"checkpoint.pth\"):\n",
                "    \"\"\"Saves checkpoint directly to S3\"\"\"\n",
                "    s3_uri = f\"s3://{BUCKET_NAME}/style-sync/models/{filename}\"\n",
                "    print(f\"Saving checkpoint to {s3_uri}...\")\n",
                "    \n",
                "    # Save to buffer first\n",
                "    buffer = io.BytesIO()\n",
                "    torch.save(state, buffer)\n",
                "    buffer.seek(0)\n",
                "    \n",
                "    # Upload buffer to S3\n",
                "    # We use the same session/client approach if needed, but smart_open usually handles it if env vars are correct\n",
                "    # To be safe, let's use the explicit client again\n",
                "    session = boto3.Session(\n",
                "        aws_access_key_id=os.environ['AWS_ACCESS_KEY_ID'],\n",
                "        aws_secret_access_key=os.environ['AWS_SECRET_ACCESS_KEY'],\n",
                "        region_name=os.environ['AWS_REGION']\n",
                "    )\n",
                "    with s_open(s3_uri, 'wb', transport_params={'client': session.client('s3')}) as f:\n",
                "        f.write(buffer.read())\n",
                "    print(\"Checkpoint saved.\")\n",
                "\n",
                "def train_model(model, dataloader, criterion, optimizer, num_epochs=10):\n",
                "    model.train()\n",
                "    \n",
                "    for epoch in range(num_epochs):\n",
                "        running_loss = 0.0\n",
                "        loop = tqdm(dataloader, total=len(dataloader), leave=True)\n",
                "        loop.set_description(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
                "        \n",
                "        for images, labels in loop:\n",
                "            images = images.to(device)\n",
                "            labels = labels.to(device)\n",
                "            \n",
                "            # Forward pass\n",
                "            outputs = model(images)\n",
                "            loss = criterion(outputs, labels)\n",
                "            \n",
                "            # Backward pass\n",
                "            optimizer.zero_grad()\n",
                "            loss.backward()\n",
                "            optimizer.step()\n",
                "            \n",
                "            running_loss += loss.item()\n",
                "            loop.set_postfix(loss=loss.item())\n",
                "        \n",
                "        epoch_loss = running_loss / len(dataloader)\n",
                "        print(f\"Epoch {epoch+1} Loss: {epoch_loss:.4f}\")\n",
                "        \n",
                "        # Save Checkpoint every epoch\n",
                "        checkpoint = {\n",
                "            'epoch': epoch + 1,\n",
                "            'state_dict': model.state_dict(),\n",
                "            'optimizer': optimizer.state_dict(),\n",
                "            'classes': CLASSES\n",
                "        }\n",
                "        save_checkpoint(checkpoint, filename=f\"resnet50_epoch_{epoch+1}.pth\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cell 7: Execution\n",
                "\n",
                "# Hyperparameters\n",
                "BATCH_SIZE = 32\n",
                "LEARNING_RATE = 1e-4\n",
                "EPOCHS = 10\n",
                "\n",
                "# DataLoader\n",
                "# num_workers=2 is usually safe for S3 streaming; too many might cause timeouts\n",
                "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
                "\n",
                "# Loss and Optimizer\n",
                "criterion = nn.BCEWithLogitsLoss()\n",
                "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
                "\n",
                "# Start Training\n",
                "train_model(model, dataloader, criterion, optimizer, num_epochs=EPOCHS)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
